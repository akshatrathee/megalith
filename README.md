# Kheti AI - Distributed Model Mesh

**Single API endpoint for all your AI models** - local and cloud.

Smart orchestration across your homelab GPUs, CPUs, NPUs, and cloud APIs with intelligent routing, redundancy, and cost optimization.

ğŸŒ **OpenAI-compatible API** â€¢ ğŸš€ **Auto-routing** â€¢ ğŸ’° **Cost-optimized** â€¢ ğŸ”’ **Tailscale-secured**

---

## âš¡ Quick Start (5 Minutes)

### On Pi5L (or any orchestrator machine):

```bash
# Clone the repository
git clone https://github.com/akshatrathee/kheti-ai.git
cd kheti-ai

# Initial setup (interactive - configures API keys)
chmod +x setup.sh
./setup.sh

# That's it! Your orchestrator is running.
```

**Access your API at:** `https://pi5l.tailf49db2.ts.net/llm`

---

## ğŸ“š Documentation

| Document | Purpose |
|----------|---------|
| **[MASTER_README.md](MASTER_README.md)** | Complete guide & architecture |
| **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)** | Daily cheat sheet |
| **[MODEL_DISTRIBUTION.md](MODEL_DISTRIBUTION.md)** | Hardware specs & model assignments |
| **[TAILSCALE_SETUP.md](TAILSCALE_SETUP.md)** | Remote access configuration |

---

## ğŸ—ï¸ Architecture

```
Your Apps (VSCode, Python, etc.)
         â†“
LiteLLM Orchestrator (Pi5L)
    â†“           â†“            â†“
Local Ollama   Cloud APIs   NPU/Metal
(RTX 5090)     (Claude/GPT) (Experimental)
```

**Naming Convention:** `SIZE-LOCATION-COST-MODEL_NAME`
- `XL-LOC-FRE-qwen2.5-coder-32b` = 32B code model on local GPU
- `S-CLD-PAY-gpt-4o-mini` = Small cloud model (paid)

---

## ğŸ¯ Features

âœ… **Single API** - One endpoint for all models  
âœ… **Smart Routing** - Weighted by speed (40%), quality (35%), cost (25%)  
âœ… **Auto-failover** - Redundancy across machines  
âœ… **Response Caching** - Save API costs  
âœ… **Multi-modal** - Code, vision, voice, image generation  
âœ… **OpenAI Compatible** - Drop-in replacement  

---

## ğŸ“¦ What's Included

- **LiteLLM Orchestrator** - Smart routing proxy
- **PostgreSQL** - Request logging & model configs
- **Redis** - Response caching
- **Grafana + Prometheus** - Monitoring (optional)
- **Tailscale Integration** - Secure remote access
- **Model Templates** - Ready for 19+ machines

---

## ğŸ® Usage

### Python

```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_MASTER_KEY",
    base_url="https://pi5l.tailf49db2.ts.net/llm"
)

# Auto-routes to best local model
response = client.chat.completions.create(
    model="code",
    messages=[{"role": "user", "content": "Write Rust code"}]
)
```

### cURL

```bash
curl https://pi5l.tailf49db2.ts.net/llm/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_KEY" \
  -d '{
    "model": "fast",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### VSCode (Continue.dev)

```json
{
  "models": [{
    "provider": "openai",
    "model": "code",
    "apiBase": "https://pi5l.tailf49db2.ts.net/llm",
    "apiKey": "YOUR_MASTER_KEY"
  }]
}
```

---

## ğŸ”„ Updates

Pull latest changes and redeploy:

```bash
cd kheti-ai
git pull
docker-compose down
docker-compose up -d
```

---

## ğŸ› ï¸ Current Setup

### âœ… Configured & Ready
- **Akshat-PC (RTX 5090):** 54 models mapped
- **Cloud APIs:** Templates ready
- **Pi5L:** Orchestrator deployment ready

### â­ Add When Ready
- **11 Mini PCs:** Templates in config (commented)
- **1 Mac Mini:** Template ready
- **11 Raspberry Pis:** For monitoring

See [MODEL_DISTRIBUTION.md](MODEL_DISTRIBUTION.md) for details.

---

## ğŸ”§ Common Commands

```bash
# Check status
docker ps
docker logs -f litellm-proxy

# Restart
docker restart litellm-proxy

# Update config
nano litellm_config_production.yaml
docker restart litellm-proxy

# View costs
# Open: https://pi5l.tailf49db2.ts.net/llm/ui
```

---

## ğŸ“ Hardware Requirements

**Orchestrator (Pi5L):**
- Raspberry Pi 5 (4GB+) OR
- Mini PC (4GB+ RAM)
- Docker installed
- Tailscale configured

**Ollama Machines:**
- Any machine with Ollama installed
- Network accessible (Tailscale recommended)

---

## ğŸ” Security

- API key required for all requests
- Tailscale for secure remote access
- Cloud API keys stored in `.env` (not committed)
- Change default passwords before production

---

## ğŸ’° Cost

- **Local models:** Free (electricity only)
- **Cloud APIs:** Pay per token
- **Budget limit:** Configurable (default $100/month)
- **Monitoring:** Track costs in admin UI

---

## ğŸ“„ License

MIT License - See LICENSE file

---

## ğŸ™ Credits

Built with:
- [LiteLLM](https://github.com/BerriAI/litellm) - Model orchestration
- [Ollama](https://ollama.ai) - Local model serving
- [Tailscale](https://tailscale.com) - Secure networking

---

## ğŸ“ Support

- **Issues:** [GitHub Issues](https://github.com/akshatrathee/kheti-ai/issues)
- **Docs:** See [MASTER_README.md](MASTER_README.md)
- **Quick Help:** See [QUICK_REFERENCE.md](QUICK_REFERENCE.md)

---

## ğŸš€ Next Steps

1. âœ… Clone repo
2. âœ… Run `./setup.sh`
3. â­ Add more Ollama machines (see [MODEL_DISTRIBUTION.md](MODEL_DISTRIBUTION.md))
4. â­ Enable monitoring (run `./deploy.sh` â†’ option 2)
5. â­ Integrate with your apps

---

**Your homelab, your models, your API.** ğŸ¯
