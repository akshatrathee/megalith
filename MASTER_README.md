# LiteLLM Model Mesh - Complete Deployment Guide
## Akshat's Distributed AI Infrastructure

**Status:** Production-Ready  
**Orchestrator:** Pi5L (100.77.119.64 / 10.0.0.111)  
**Access:** https://pi5l.tailf49db2.ts.net/llm

---

## ğŸ“ File Structure

```
deployment-package/
â”œâ”€â”€ README.md                          â† This file
â”œâ”€â”€ MODEL_DISTRIBUTION.md              â† Hardware assignments & model list
â”œâ”€â”€ litellm_config_production.yaml    â† Main config (with templates)
â”œâ”€â”€ docker-compose.yml                 â† Docker stack definition
â”œâ”€â”€ .env.example                       â† API keys template
â”œâ”€â”€ deploy.sh                          â† One-command deployment
â”œâ”€â”€ TAILSCALE_SETUP.md                â† Remote access setup
â”œâ”€â”€ QUICK_REFERENCE.md                â† Cheat sheet
â””â”€â”€ test_client.py                     â† Testing script
```

---

## ğŸ¯ What You're Building

A **single API endpoint** that intelligently routes requests across:
- **Your local Ollama machines** (currently Akshat-PC with RTX 5090)
- **Future machines** (11 mini PCs, 1 Mac Mini, 12 Raspberry Pis)
- **Cloud APIs** (Claude, GPT, Gemini, Perplexity)

**Key Features:**
- âœ… OpenAI-compatible API (drop-in replacement)
- âœ… Smart weighted routing (speed 40%, quality 35%, cost 25%)
- âœ… Automatic redundancy and failover
- âœ… Response caching (save API costs)
- âœ… Works locally AND remotely via Tailscale
- âœ… Template-based config for easy machine additions

**Model Naming Convention:**
```
FORMAT: SIZE-LOCATION-COST-MODEL_NAME

Examples:
  XL-LOC-FRE-qwen2.5-coder-32b    â†’ Large local free model
  S-CLD-PAY-gpt-4o-mini            â†’ Small cloud paid model
  M-LOC-FRE-llama3.2-vision-11b    â†’ Medium local free vision model
```

---

## ğŸš€ Quick Start (30 Minutes)

### Step 1: Deploy Orchestrator on Pi5L

```bash
# 1. Copy files to Pi5L
scp -r deployment-package/* pi@10.0.0.111:~/litellm-mesh/

# 2. SSH to Pi5L
ssh pi@10.0.0.111

# 3. Go to directory
cd ~/litellm-mesh

# 4. Setup API keys
cp .env.example .env
nano .env
# Add your cloud API keys (Claude, GPT, Gemini, Perplexity)

# 5. Run deployment
chmod +x deploy.sh
./deploy.sh

# Choose:
# - Option 1: Basic (recommended for Pi5)
# - Configure Tailscale serve: Yes

# 6. Verify
curl http://localhost:4000/health
curl https://pi5l.tailf49db2.ts.net/llm/health
```

**That's it!** Your orchestrator is running. âœ…

### Step 2: Your Existing Ollama Machine (Already Done)

Your Akshat-PC (RTX 5090) at `100.111.115.92:11434` is already configured and will work immediately!

The config file already includes all your existing models with proper naming:
- `XL-LOC-FRE-qwen2.5-coder-32b`
- `XL-LOC-FRE-deepseek-coder-236b`
- `M-LOC-FRE-llama3.2-vision-11b`
- `L-LOC-FRE-whisper-large-v3`
- And 50+ more!

### Step 3: Add More Machines (Optional, When Ready)

See **MODEL_DISTRIBUTION.md** for which models to install on which machines.

For each new machine:
```bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Configure for network access
export OLLAMA_HOST=0.0.0.0:11434
sudo systemctl restart ollama

# 3. Pull assigned models (see MODEL_DISTRIBUTION.md)
ollama pull <model-name>

# 4. Update litellm_config_production.yaml
# Uncomment the machine's section
# Add Tailscale IP
# Restart LiteLLM: docker restart litellm-proxy
```

---

## ğŸ“– Documentation Guide

| Document | Purpose | When to Read |
|----------|---------|--------------|
| **README.md** (this file) | Overview & quick start | Start here |
| **QUICK_REFERENCE.md** | Cheat sheet, common commands | Daily use |
| **MODEL_DISTRIBUTION.md** | Hardware specs & model assignments | When adding machines |
| **TAILSCALE_SETUP.md** | Remote access configuration | For Tailscale setup |
| **litellm_config_production.yaml** | Full config with templates | When customizing |

---

## ğŸ® Using Your Model Mesh

### Python (OpenAI SDK)

```python
from openai import OpenAI

# Connect to your homelab
client = OpenAI(
    api_key="YOUR_MASTER_KEY",
    base_url="https://pi5l.tailf49db2.ts.net/llm"
)

# Fast local chat
response = client.chat.completions.create(
    model="gpt-3.5-turbo",  # Routes to fast local model
    messages=[{"role": "user", "content": "Hello!"}]
)

# Best code generation
response = client.chat.completions.create(
    model="code",  # Routes to XL-LOC-FRE-qwen2.5-coder-32b
    messages=[{"role": "user", "content": "Write binary search in Rust"}]
)

# Vision task
response = client.chat.completions.create(
    model="vision",  # Routes to XL-LOC-FRE-llava-34b
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
        ]
    }]
)

# Embeddings
response = client.embeddings.create(
    model="text-embedding-ada-002",
    input="Your text here"
)
```

### cURL

```bash
curl https://pi5l.tailf49db2.ts.net/llm/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_KEY" \
  -d '{
    "model": "code",
    "messages": [{"role": "user", "content": "Write quicksort"}]
  }'
```

### Continue.dev (VSCode)

```json
{
  "models": [{
    "title": "Homelab XL Code",
    "provider": "openai",
    "model": "code",
    "apiBase": "https://pi5l.tailf49db2.ts.net/llm",
    "apiKey": "YOUR_MASTER_KEY"
  }]
}
```

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Applications                                       â”‚
â”‚  (VSCode, Python scripts, N8N, etc.)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LiteLLM Orchestrator (Pi5L)                            â”‚
â”‚  - Smart routing based on prompt analysis               â”‚
â”‚  - Weighted scoring (speed, quality, cost)              â”‚
â”‚  - Response caching                                      â”‚
â”‚  - Request logging                                       â”‚
â”‚  https://pi5l.tailf49db2.ts.net/llm                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼          â–¼          â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Akshat-PC  â”‚ â”‚ GTR9   â”‚ â”‚ SER9   â”‚ â”‚Mac Miniâ”‚ â”‚ Cloud  â”‚
â”‚ RTX 5090   â”‚ â”‚ CPU    â”‚ â”‚ NPU    â”‚ â”‚ Metal  â”‚ â”‚ APIs   â”‚
â”‚ XL models  â”‚ â”‚128GB   â”‚ â”‚ Fast   â”‚ â”‚ M4 Pro â”‚ â”‚Premium â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Current Status

### âœ… Already Configured & Working
- **Akshat-PC (RTX 5090):** 54 models, fully integrated
- **Cloud APIs:** Ready (just add keys to .env)
- **Pi5L Orchestrator:** Ready to deploy

### â­ Ready to Add (When You Want)
- **11 Mini PCs:** Templates ready in config
- **1 Mac Mini:** Template ready
- **11 Raspberry Pis:** Can use for monitoring

---

## ğŸ”§ Common Tasks

### Check Status
```bash
# On Pi5L
docker ps
docker logs litellm-proxy

# Test health
curl https://pi5l.tailf49db2.ts.net/llm/health
```

### Add New Machine
```bash
# 1. See MODEL_DISTRIBUTION.md for model assignments
# 2. Install Ollama and pull models on new machine
# 3. Edit litellm_config_production.yaml
#    - Find machine section (commented)
#    - Uncomment it
#    - Replace [TO_BE_CONFIGURED] with Tailscale IP
# 4. Restart: docker restart litellm-proxy
```

### View Logs
```bash
# LiteLLM
docker logs -f litellm-proxy

# PostgreSQL
docker logs -f litellm-postgres

# All services
docker-compose logs -f
```

### Update Configuration
```bash
# Edit config
nano litellm_config_production.yaml

# Restart to apply
docker restart litellm-proxy
```

### Backup Everything
```bash
tar -czf litellm-backup-$(date +%F).tar.gz \
  litellm_config_production.yaml \
  .env \
  docker-compose.yml \
  litellm_data/
```

---

## ğŸ¯ Model Selection Strategy

| Use Case | Start With | Escalate To | Final Fallback |
|----------|-----------|-------------|----------------|
| Quick chat | `fast` (S-LOC) | `balanced` (M-LOC) | `gpt-3.5-turbo` (Cloud) |
| Code generation | `code` (XL-LOC) | `code` (XL-LOC) | `gpt-4` (Cloud) |
| Vision/images | `vision` (XL-LOC) | `vision` (XL-LOC) | `gpt-4o` (Cloud) |
| Complex reasoning | `best` (XL-LOC) | `best-cloud` (Cloud) | `claude-opus` (Cloud) |
| Research/web | N/A | N/A | `research` (Cloud) |

---

## ğŸ” Security Notes

1. **Change default passwords** in `litellm_config_production.yaml`:
   - `master_key`
   - `ui_password`

2. **Tailscale access:** Anyone on your Tailnet can reach the API

3. **API key required:** All requests need `Authorization: Bearer <key>`

4. **Environment variables:** Never commit `.env` to git

---

## ğŸ’° Cost Tracking

- **Local models:** Free (just electricity)
- **Cloud models:** Pay per token
- **Monthly budget:** Set in config (default $100/month)
- **View costs:** Admin UI â†’ https://pi5l.tailf49db2.ts.net/llm/ui

---

## ğŸ“ Next Steps

### Immediate (Day 1)
1. âœ… Deploy orchestrator on Pi5L
2. âœ… Test with existing Akshat-PC models
3. âœ… Set up Python client in your projects

### Short-term (Week 1)
1. â­ Add one more GPU machine (Old Office PC)
2. â­ Set up monitoring (Grafana)
3. â­ Fine-tune routing weights based on usage

### Long-term (Month 1)
1. â­ Add high-RAM CPU machines (GTR9, EVO-X2, MS-S1)
2. â­ Experiment with NPU machines (SER9, Atomman)
3. â­ Add Mac Mini for Metal optimization
4. â­ Set up automated model updates

---

## ğŸ“ Troubleshooting

### "Model not found"
```bash
# Check if model exists in config
grep "MODEL_NAME" litellm_config_production.yaml

# Check if Ollama machine is reachable
curl http://MACHINE_IP:11434/api/tags
```

### Slow responses
```bash
# Check which models are loaded
ssh user@ollama-machine
ollama ps

# Preload model
ollama run MODEL_NAME
```

### Can't access via Tailscale
```bash
# Check Tailscale status
sudo tailscale status

# Check serve configuration
sudo tailscale serve status

# Reconfigure if needed
sudo tailscale serve --bg --https=443 /llm http://localhost:4000
```

---

## ğŸ“š Advanced Configuration

### Custom Routing Rules

Edit `litellm_config_production.yaml`:

```yaml
router_settings:
  routing_strategy_args:
    ttft_weight: 0.5      # Prioritize speed more
    quality_weight: 0.3
    cost_weight: 0.2
```

### Add Custom Model Alias

```yaml
router_settings:
  model_group_alias:
    my-custom-name: XL-LOC-FRE-qwen2.5-coder-32b
```

### Enable Monitoring

```bash
# Redeploy with monitoring
./deploy.sh
# Choose option 2: Full (with monitoring)

# Access Grafana
https://pi5l.tailf49db2.ts.net:3000
```

---

## ğŸ‰ Success Criteria

You'll know it's working when:

âœ… Health check returns `{"status": "healthy"}`  
âœ… Admin UI loads at `/ui`  
âœ… Test chat completes in <5 seconds  
âœ… Models list shows your Ollama models  
âœ… Can access via both Tailscale and local LAN  
âœ… Python client connects successfully  

---

## ğŸ“¦ Package Contents Summary

| File | Size | Purpose |
|------|------|---------|
| docker-compose.yml | 3 KB | Service definitions |
| litellm_config_production.yaml | 25 KB | Complete routing config |
| MODEL_DISTRIBUTION.md | 15 KB | Hardware & model guide |
| QUICK_REFERENCE.md | 12 KB | Daily cheat sheet |
| TAILSCALE_SETUP.md | 8 KB | Remote access guide |
| deploy.sh | 4 KB | Deployment automation |
| test_client.py | 5 KB | Testing script |
| .env.example | 1 KB | API keys template |

**Total:** Ready-to-deploy production infrastructure

---

## ğŸ† What You Get

A **production-grade AI infrastructure** with:

- ğŸš€ **Single API** for all your models
- ğŸ¯ **Smart routing** based on task requirements  
- ğŸ’ª **Redundancy** across multiple machines
- ğŸ’° **Cost optimization** (local-first, cloud-fallback)
- ğŸ”’ **Secure** Tailscale access
- ğŸ“Š **Monitoring** (optional)
- ğŸ”§ **Easy scaling** (just uncomment templates)

**No vendor lock-in.** OpenAI-compatible API. Your hardware, your control.

---

**Questions?** Check QUICK_REFERENCE.md for common commands and patterns.

**Ready to deploy?** Run `./deploy.sh` on Pi5L! ğŸš€
