# LiteLLM Configuration - Akshat's Distributed Model Mesh
# Naming: SIZE-LOCATION-COST-MODEL_NAME
# Sizes: XS (<3B), S (3-8B), M (8-20B), L (20-40B), XL (40B+)
# Location: LOC (local), CLD (cloud)
# Cost: FRE (free/local), PAY (paid cloud)

model_list:
  # ============================================
  # TIER 1: GPU POWERHOUSES
  # ============================================
  
  # -------------------------------------------
  # Akshat-PC (RTX 5090, 192GB)
  # Tailscale: 100.111.115.92
  # Local: 10.0.0.5
  # Hostname: akshat-pc.tailf49db2.ts.net
  # -------------------------------------------
  
  # XL Code Models
  - model_name: XL-LOC-FRE-qwen2.5-coder-32b
    litellm_params:
      model: ollama/qwen2.5-coder:32b
      api_base: http://100.111.115.92:11434
      num_retries: 2
      timeout: 300
    model_info:
      mode: chat
      tags: ["code", "xl", "local", "gpu", "5090"]
      tier: "performance"
      
  - model_name: XL-LOC-FRE-deepseek-coder-236b
    litellm_params:
      model: ollama/deepseek-coder-v2:236b
      api_base: http://100.111.115.92:11434
      num_retries: 2
      timeout: 600
    model_info:
      mode: chat
      tags: ["code", "xl", "local", "gpu", "best-code"]
      tier: "performance"
      
  - model_name: XL-LOC-FRE-qwen2.5-72b
    litellm_params:
      model: ollama/qwen2.5:72b
      api_base: http://100.111.115.92:11434
      num_retries: 2
      timeout: 300
    model_info:
      mode: chat
      tags: ["general", "xl", "local", "gpu"]
      tier: "performance"
      
  - model_name: XL-LOC-FRE-llava-34b
    litellm_params:
      model: ollama/llava:34b
      api_base: http://100.111.115.92:11434
      num_retries: 2
    model_info:
      mode: image_text_to_text
      tags: ["vision", "xl", "local", "gpu"]
      tier: "performance"
      
  # L Models
  - model_name: L-LOC-FRE-deepseek-r1-14b
    litellm_params:
      model: ollama/deepseek-r1:14b
      api_base: http://100.111.115.92:11434
      num_retries: 2
    model_info:
      mode: chat
      tags: ["reasoning", "l", "local", "gpu"]
      tier: "performance"
      
  - model_name: L-LOC-FRE-qwen2.5-32b
    litellm_params:
      model: ollama/qwen2.5:32b
      api_base: http://100.111.115.92:11434
      num_retries: 2
    model_info:
      mode: chat
      tags: ["general", "l", "local", "gpu"]
      tier: "performance"
      
  - model_name: L-LOC-FRE-codestral-22b
    litellm_params:
      model: ollama/codestral:22b
      api_base: http://100.111.115.92:11434
      num_retries: 2
    model_info:
      mode: chat
      tags: ["code", "l", "local", "gpu"]
      tier: "performance"
      
  # M Vision
  - model_name: M-LOC-FRE-llama3.2-vision-11b
    litellm_params:
      model: ollama/llama3.2-vision:11b
      api_base: http://100.111.115.92:11434
      num_retries: 2
    model_info:
      mode: image_text_to_text
      tags: ["vision", "m", "local", "gpu"]
      tier: "performance"
      
  # Image Generation
  - model_name: XL-LOC-FRE-sdxl
    litellm_params:
      model: ollama/stable-diffusion-xl
      api_base: http://100.111.115.92:11434
      num_retries: 2
      timeout: 180
    model_info:
      mode: image_generation
      tags: ["image-gen", "xl", "local", "gpu"]
      tier: "performance"
      
  - model_name: XL-LOC-FRE-flux-schnell
    litellm_params:
      model: ollama/flux-schnell
      api_base: http://100.111.115.92:11434
      num_retries: 2
      timeout: 120
    model_info:
      mode: image_generation
      tags: ["image-gen", "xl", "local", "gpu", "fast"]
      tier: "performance"
      
  # Voice (Whisper)
  - model_name: L-LOC-FRE-whisper-large-v3
    litellm_params:
      model: ollama/whisper:large-v3
      api_base: http://100.111.115.92:11434
      num_retries: 2
    model_info:
      mode: audio_transcription
      tags: ["voice", "l", "local", "gpu"]
      tier: "performance"

  # -------------------------------------------
  # Old-Office-PC (RTX 2080 Super, 32GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # REDUNDANCY + MID-TIER
  # - model_name: XL-LOC-FRE-qwen2.5-coder-32b-q4-backup
  #   litellm_params:
  #     model: ollama/qwen2.5-coder:32b-q4
  #     api_base: http://OFFICE_PC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["code", "xl", "local", "gpu", "backup", "2080s"]
  #     tier: "backup"
  #     
  # - model_name: XL-LOC-FRE-llama3.1-70b-q4
  #   litellm_params:
  #     model: ollama/llama3.1:70b-q4
  #     api_base: http://OFFICE_PC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xl", "local", "gpu", "2080s"]
  #     tier: "backup"
  #     
  # - model_name: M-LOC-FRE-llama3.2-vision-11b-backup
  #   litellm_params:
  #     model: ollama/llama3.2-vision:11b
  #     api_base: http://OFFICE_PC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: image_text_to_text
  #     tags: ["vision", "m", "local", "gpu", "backup"]
  #     tier: "backup"
  #     
  # - model_name: M-LOC-FRE-mistral-nemo-12b
  #   litellm_params:
  #     model: ollama/mistral-nemo:12b
  #     api_base: http://OFFICE_PC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "m", "local", "gpu"]
  #     tier: "backup"
  #     
  # - model_name: S-LOC-FRE-llama3.2-3b-2080s
  #   litellm_params:
  #     model: ollama/llama3.2:3b
  #     api_base: http://OFFICE_PC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "gpu", "fast"]
  #     tier: "backup"
  #     
  # - model_name: L-LOC-FRE-sd-2.1
  #   litellm_params:
  #     model: ollama/stable-diffusion-2.1
  #     api_base: http://OFFICE_PC_TAILSCALE_IP:11434
  #   model_info:
  #     mode: image_generation
  #     tags: ["image-gen", "l", "local", "gpu"]
  #     tier: "backup"
  #     
  # - model_name: S-LOC-FRE-nomic-embed-2080s
  #   litellm_params:
  #     model: ollama/nomic-embed-text
  #     api_base: http://OFFICE_PC_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "s", "local"]
  #     tier: "backup"

  # ============================================
  # TIER 2: HIGH-RAM CPU MONSTERS
  # ============================================
  
  # -------------------------------------------
  # GTR9-Pro (AI Max+ 395, 128GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # XL CPU-OPTIMIZED MODELS
  # - model_name: XL-LOC-FRE-qwen2.5-72b-q4-cpu
  #   litellm_params:
  #     model: ollama/qwen2.5:72b-q4
  #     api_base: http://GTR9_TAILSCALE_IP:11434
  #     num_retries: 2
  #     timeout: 600
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xl", "local", "cpu", "128gb-ram", "high-context"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: XL-LOC-FRE-llama3.1-70b-q4-cpu
  #   litellm_params:
  #     model: ollama/llama3.1:70b-q4
  #     api_base: http://GTR9_TAILSCALE_IP:11434
  #     num_retries: 2
  #     timeout: 600
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xl", "local", "cpu", "backup"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: XL-LOC-FRE-mixtral-8x22b-q4
  #   litellm_params:
  #     model: ollama/mixtral:8x22b-q4
  #     api_base: http://GTR9_TAILSCALE_IP:11434
  #     num_retries: 2
  #     timeout: 600
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xl", "local", "cpu", "moe"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: M-LOC-FRE-mxbai-embed-gtr9
  #   litellm_params:
  #     model: ollama/mxbai-embed-large
  #     api_base: http://GTR9_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "m", "local"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: M-LOC-FRE-bge-m3-gtr9
  #   litellm_params:
  #     model: ollama/bge-m3
  #     api_base: http://GTR9_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "m", "local", "multilingual"]
  #     tier: "cpu-heavy"

  # -------------------------------------------
  # EVO-X2 (AI Max+ 395, 128GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # - model_name: XL-LOC-FRE-deepseek-coder-236b-q4-cpu
  #   litellm_params:
  #     model: ollama/deepseek-coder-v2:236b-q4
  #     api_base: http://EVOX2_TAILSCALE_IP:11434
  #     num_retries: 2
  #     timeout: 900
  #   model_info:
  #     mode: chat
  #     tags: ["code", "xl", "local", "cpu", "best-code"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: XL-LOC-FRE-qwen2.5-72b-q4-cpu-backup
  #   litellm_params:
  #     model: ollama/qwen2.5:72b-q4
  #     api_base: http://EVOX2_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xl", "local", "cpu", "backup"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: S-LOC-FRE-arctic-embed-evox2
  #   litellm_params:
  #     model: ollama/snowflake-arctic-embed:335m
  #     api_base: http://EVOX2_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "s", "local"]
  #     tier: "cpu-heavy"

  # -------------------------------------------
  # MS-S1 (AI Max+ 395, 128GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # - model_name: XL-LOC-FRE-qwen2.5-72b-q4-cpu-backup2
  #   litellm_params:
  #     model: ollama/qwen2.5:72b-q4
  #     api_base: http://MSS1_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xl", "local", "cpu", "backup"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: XL-LOC-FRE-llama3.1-70b-q4-cpu-backup2
  #   litellm_params:
  #     model: ollama/llama3.1:70b-q4
  #     api_base: http://MSS1_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xl", "local", "cpu", "backup"]
  #     tier: "cpu-heavy"
  #     
  # - model_name: S-LOC-FRE-granite-embed-mss1
  #   litellm_params:
  #     model: ollama/granite-embedding
  #     api_base: http://MSS1_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "s", "local"]
  #     tier: "cpu-heavy"

  # ============================================
  # TIER 3: NPU-CAPABLE (Experimental)
  # ============================================
  
  # -------------------------------------------
  # SER9 (AMD XDNA 2 NPU, 32GB, Ryzen AI 9)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # NPU-OPTIMIZED MODELS
  # - model_name: S-LOC-FRE-phi3.5-3.8b-npu
  #   litellm_params:
  #     model: ollama/phi3.5:3.8b
  #     api_base: http://SER9_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "npu", "amd-xdna2", "experimental"]
  #     tier: "npu"
  #     
  # - model_name: S-LOC-FRE-llama3.2-3b-npu
  #   litellm_params:
  #     model: ollama/llama3.2:3b
  #     api_base: http://SER9_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "npu", "fast"]
  #     tier: "npu"
  #     
  # - model_name: M-LOC-FRE-qwen2.5-14b-ser9
  #   litellm_params:
  #     model: ollama/qwen2.5:14b
  #     api_base: http://SER9_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "m", "local"]
  #     tier: "npu"
  #     
  # - model_name: S-LOC-FRE-mistral-7b-ser9
  #   litellm_params:
  #     model: ollama/mistral:7b
  #     api_base: http://SER9_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local"]
  #     tier: "npu"

  # -------------------------------------------
  # Atomman-X7-Ti (Intel Meteor Lake NPU, 32GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # - model_name: S-LOC-FRE-phi3.5-3.8b-intel-npu
  #   litellm_params:
  #     model: ollama/phi3.5:3.8b
  #     api_base: http://ATOMMAN_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "npu", "intel-meteor-lake", "experimental"]
  #     tier: "npu"
  #     
  # - model_name: S-LOC-FRE-llama3.2-3b-intel-npu
  #   litellm_params:
  #     model: ollama/llama3.2:3b
  #     api_base: http://ATOMMAN_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "npu", "fast"]
  #     tier: "npu"
  #     
  # - model_name: M-LOC-FRE-qwen2.5-14b-atomman
  #   litellm_params:
  #     model: ollama/qwen2.5:14b
  #     api_base: http://ATOMMAN_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "m", "local"]
  #     tier: "npu"
  #     
  # - model_name: M-LOC-FRE-codellama-13b-atomman
  #   litellm_params:
  #     model: ollama/codellama:13b
  #     api_base: http://ATOMMAN_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["code", "m", "local"]
  #     tier: "npu"
  #     
  # - model_name: S-LOC-FRE-nomic-embed-atomman
  #   litellm_params:
  #     model: ollama/nomic-embed-text
  #     api_base: http://ATOMMAN_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "s", "local"]
  #     tier: "npu"

  # ============================================
  # TIER 4: APPLE SILICON
  # ============================================
  
  # -------------------------------------------
  # Mac-Mini-M4-Pro (M4 Pro 12-core, 24GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # METAL-OPTIMIZED MODELS
  # - model_name: M-LOC-FRE-qwen2.5-14b-metal
  #   litellm_params:
  #     model: ollama/qwen2.5:14b
  #     api_base: http://MACMINI_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "m", "local", "metal", "apple-silicon"]
  #     tier: "apple"
  #     
  # - model_name: S-LOC-FRE-llama3.2-3b-metal
  #   litellm_params:
  #     model: ollama/llama3.2:3b
  #     api_base: http://MACMINI_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "metal", "fast"]
  #     tier: "apple"
  #     
  # - model_name: S-LOC-FRE-phi3.5-metal
  #   litellm_params:
  #     model: ollama/phi3.5:3.8b
  #     api_base: http://MACMINI_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "metal"]
  #     tier: "apple"
  #     
  # - model_name: M-LOC-FRE-sd-coreml
  #   litellm_params:
  #     model: ollama/stable-diffusion
  #     api_base: http://MACMINI_TAILSCALE_IP:11434
  #   model_info:
  #     mode: image_generation
  #     tags: ["image-gen", "m", "local", "coreml"]
  #     tier: "apple"
  #     
  # - model_name: S-LOC-FRE-nomic-embed-mac
  #   litellm_params:
  #     model: ollama/nomic-embed-text
  #     api_base: http://MACMINI_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "s", "local"]
  #     tier: "apple"

  # ============================================
  # TIER 5: MID-RANGE CPU
  # ============================================
  
  # -------------------------------------------
  # MS01 (i9-12900H, assumed 32-64GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # - model_name: M-LOC-FRE-qwen2.5-14b-ms01
  #   litellm_params:
  #     model: ollama/qwen2.5:14b
  #     api_base: http://MS01_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "m", "local"]
  #     tier: "mid"
  #     
  # - model_name: M-LOC-FRE-codellama-13b-ms01
  #   litellm_params:
  #     model: ollama/codellama:13b
  #     api_base: http://MS01_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["code", "m", "local"]
  #     tier: "mid"
  #     
  # - model_name: M-LOC-FRE-mistral-nemo-12b-ms01
  #   litellm_params:
  #     model: ollama/mistral-nemo:12b
  #     api_base: http://MS01_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "m", "local"]
  #     tier: "mid"
  #     
  # - model_name: S-LOC-FRE-llama3.2-3b-ms01
  #   litellm_params:
  #     model: ollama/llama3.2:3b
  #     api_base: http://MS01_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local", "fast"]
  #     tier: "mid"
  #     
  # - model_name: S-LOC-FRE-deepseek-r1-8b-ms01
  #   litellm_params:
  #     model: ollama/deepseek-r1:8b
  #     api_base: http://MS01_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["reasoning", "s", "local"]
  #     tier: "mid"

  # -------------------------------------------
  # BorgCube (i5-12400, 32GB DDR5)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # - model_name: S-LOC-FRE-qwen2.5-7b-borgcube
  #   litellm_params:
  #     model: ollama/qwen2.5:7b
  #     api_base: http://BORGCUBE_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local"]
  #     tier: "mid"
  #     
  # - model_name: S-LOC-FRE-mistral-7b-borgcube
  #   litellm_params:
  #     model: ollama/mistral:7b
  #     api_base: http://BORGCUBE_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local"]
  #     tier: "mid"
  #     
  # - model_name: S-LOC-FRE-phi3.5-3.8b-borgcube
  #   litellm_params:
  #     model: ollama/phi3.5:3.8b
  #     api_base: http://BORGCUBE_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local"]
  #     tier: "mid"
  #     
  # - model_name: S-LOC-FRE-nomic-embed-borgcube
  #   litellm_params:
  #     model: ollama/nomic-embed-text
  #     api_base: http://BORGCUBE_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "s", "local"]
  #     tier: "mid"

  # -------------------------------------------
  # RetroMac (Ryzen 3 3200U, 16GB)
  # Tailscale: [TO_BE_CONFIGURED]
  # Local: [TO_BE_CONFIGURED]
  # Status: TEMPLATE - Uncomment and configure when ready
  # -------------------------------------------
  
  # TINY MODELS ONLY
  # - model_name: XS-LOC-FRE-tinyllama-1.1b
  #   litellm_params:
  #     model: ollama/tinyllama:1.1b
  #     api_base: http://RETROMAC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xs", "local", "ultra-fast"]
  #     tier: "light"
  #     
  # - model_name: S-LOC-FRE-phi2-2.7b
  #   litellm_params:
  #     model: ollama/phi-2:2.7b
  #     api_base: http://RETROMAC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "s", "local"]
  #     tier: "light"
  #     
  # - model_name: XS-LOC-FRE-llama3.2-1b
  #   litellm_params:
  #     model: ollama/llama3.2:1b
  #     api_base: http://RETROMAC_TAILSCALE_IP:11434
  #     num_retries: 2
  #   model_info:
  #     mode: chat
  #     tags: ["general", "xs", "local", "ultra-fast"]
  #     tier: "light"
  #     
  # - model_name: S-LOC-FRE-nomic-embed-retromac
  #   litellm_params:
  #     model: ollama/nomic-embed-text
  #     api_base: http://RETROMAC_TAILSCALE_IP:11434
  #   model_info:
  #     mode: embedding
  #     tags: ["embeddings", "s", "local"]
  #     tier: "light"

  # ============================================
  # CLOUD MODELS (Fallback/Premium)
  # ============================================
  
  # Claude (Anthropic)
  - model_name: XL-CLD-PAY-claude-opus-4.5
    litellm_params:
      model: claude-opus-4-5-20251101
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "xl", "reasoning", "vision", "best-quality"]
      tier: "cloud-premium"
      
  - model_name: M-CLD-PAY-claude-sonnet-4.5
    litellm_params:
      model: claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "m", "reasoning", "vision", "balanced"]
      tier: "cloud-balanced"
      
  - model_name: S-CLD-PAY-claude-haiku-4.5
    litellm_params:
      model: claude-haiku-4-5-20251001
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "s", "fast", "cheap"]
      tier: "cloud-fast"

  # OpenAI (ChatGPT)
  - model_name: XL-CLD-PAY-gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "xl", "vision", "reasoning", "best-quality"]
      tier: "cloud-premium"
      
  - model_name: S-CLD-PAY-gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "s", "fast", "cheap"]
      tier: "cloud-fast"
      
  - model_name: XL-CLD-PAY-o1
    litellm_params:
      model: o1
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "xl", "reasoning", "best-quality", "math"]
      tier: "cloud-premium"

  # Google (Gemini)
  - model_name: M-CLD-PAY-gemini-flash-2.0
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "m", "fast", "vision"]
      tier: "cloud-fast"
      
  - model_name: M-CLD-PAY-gemini-pro-1.5
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "m", "balanced", "vision"]
      tier: "cloud-balanced"

  # Perplexity
  - model_name: L-CLD-PAY-perplexity-online
    litellm_params:
      model: perplexity/llama-3.1-sonar-large-128k-online
      api_key: os.environ/PERPLEXITY_API_KEY
    model_info:
      mode: chat
      tags: ["cloud", "l", "research", "web-search"]
      tier: "cloud-specialized"

# ============================================
# ROUTER SETTINGS
# ============================================
router_settings:
  routing_strategy: usage-based-routing-v2
  routing_strategy_args:
    ttft_weight: 0.4          # Speed: 40%
    quality_weight: 0.35       # Model capability: 35%
    cost_weight: 0.25          # Efficiency/cost: 25%
  
  # Model group aliases for compatibility
  model_group_alias:
    # OpenAI compatibility
    gpt-4: XL-CLD-PAY-gpt-4o
    gpt-4-turbo: XL-CLD-PAY-gpt-4o
    gpt-3.5-turbo: S-LOC-FRE-llama3.2-3b-ms01  # Fast local
    o1: XL-CLD-PAY-o1
    
    # Anthropic compatibility
    claude-opus: XL-CLD-PAY-claude-opus-4.5
    claude-sonnet: M-CLD-PAY-claude-sonnet-4.5
    claude-haiku: S-CLD-PAY-claude-haiku-4.5
    
    # Embeddings
    text-embedding-ada-002: S-LOC-FRE-nomic-embed-2080s
    text-embedding-3-large: M-LOC-FRE-mxbai-embed-gtr9
    
    # Custom shortcuts
    best: XL-LOC-FRE-qwen2.5-coder-32b  # Best local
    best-cloud: XL-CLD-PAY-claude-opus-4.5
    fast: S-LOC-FRE-llama3.2-3b-ms01
    balanced: M-LOC-FRE-qwen2.5-14b-ms01
    code: XL-LOC-FRE-qwen2.5-coder-32b
    code-xl: XL-LOC-FRE-deepseek-coder-236b
    vision: XL-LOC-FRE-llava-34b
    vision-fast: M-LOC-FRE-llama3.2-vision-11b
    reasoning: L-LOC-FRE-deepseek-r1-14b
    research: L-CLD-PAY-perplexity-online
    image-gen: XL-LOC-FRE-flux-schnell
    voice: L-LOC-FRE-whisper-large-v3
  
  # Fallback and retry settings
  num_retries: 2
  timeout: 300
  allowed_fails: 3
  cooldown_time: 60
  
  # Caching
  redis_host: redis
  redis_port: 6379
  cache_responses: true
  cache_kwargs:
    ttl: 3600  # 1 hour

# ============================================
# LITELLM SETTINGS
# ============================================
litellm_settings:
  drop_params: true
  set_verbose: false
  json_logs: true
  
  # Callbacks
  success_callback: ["redis"]
  failure_callback: ["redis"]
  
  # Response caching
  cache: true
  cache_params:
    type: "redis"
    supported_call_types: ["completion", "acompletion", "embedding", "aembedding"]
  
  # Budget tracking
  max_budget: 100  # dollars/month
  budget_duration: "30d"

# ============================================
# GENERAL SETTINGS
# ============================================
general_settings:
  master_key: sk-akshat-homelab-key-change-this  # CHANGE THIS!
  database_url: postgresql://litellm:litellm_password@postgres:5432/litellm
  store_model_in_db: true
  
  # Admin UI
  ui_username: admin
  ui_password: admin  # CHANGE THIS!
  
  # Rate limits
  default_max_rpm: 60
  
  # Alerting
  alerting: ["webhook"]
  alerting_threshold: 5
